caffe(99642,0x7fffa48523c0) malloc: *** malloc_zone_unregister() failed for 0x7fffa4848000
I1130 19:45:21.962568 2760188864 caffe.cpp:210] Use CPU.
I1130 19:45:21.964673 2760188864 solver.cpp:48] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 10000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 1000
snapshot_prefix: "examples/mnist/lenet"
solver_mode: CPU
net: "examples/mnist/lenet_train_math_test.prototxt"
train_state {
  level: 0
  stage: ""
}
I1130 19:45:21.965214 2760188864 solver.cpp:91] Creating training net from net file: examples/mnist/lenet_train_math_test.prototxt
I1130 19:45:21.965597 2760188864 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I1130 19:45:21.965618 2760188864 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1130 19:45:21.965627 2760188864 net.cpp:58] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/imagenet/math_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 19
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1130 19:45:21.965793 2760188864 layer_factory.hpp:77] Creating layer mnist
I1130 19:45:21.972633 2760188864 net.cpp:100] Creating Layer mnist
I1130 19:45:21.972662 2760188864 net.cpp:408] mnist -> data
I1130 19:45:21.972690 2760188864 net.cpp:408] mnist -> label
I1130 19:45:21.972851 164999168 db_lmdb.cpp:35] Opened lmdb examples/imagenet/math_train_lmdb
I1130 19:45:21.972957 2760188864 data_layer.cpp:41] output data size: 64,3,32,72
I1130 19:45:21.976178 2760188864 net.cpp:150] Setting up mnist
I1130 19:45:21.976207 2760188864 net.cpp:157] Top shape: 64 3 32 72 (442368)
I1130 19:45:21.976217 2760188864 net.cpp:157] Top shape: 64 (64)
I1130 19:45:21.976223 2760188864 net.cpp:165] Memory required for data: 1769728
I1130 19:45:21.976236 2760188864 layer_factory.hpp:77] Creating layer conv1
I1130 19:45:21.976258 2760188864 net.cpp:100] Creating Layer conv1
I1130 19:45:21.976266 2760188864 net.cpp:434] conv1 <- data
I1130 19:45:21.976279 2760188864 net.cpp:408] conv1 -> conv1
I1130 19:45:21.976397 2760188864 net.cpp:150] Setting up conv1
I1130 19:45:21.976407 2760188864 net.cpp:157] Top shape: 64 20 28 68 (2437120)
I1130 19:45:21.976414 2760188864 net.cpp:165] Memory required for data: 11518208
I1130 19:45:21.976426 2760188864 layer_factory.hpp:77] Creating layer pool1
I1130 19:45:21.976457 2760188864 net.cpp:100] Creating Layer pool1
I1130 19:45:21.976465 2760188864 net.cpp:434] pool1 <- conv1
I1130 19:45:21.976475 2760188864 net.cpp:408] pool1 -> pool1
I1130 19:45:21.976490 2760188864 net.cpp:150] Setting up pool1
I1130 19:45:21.976496 2760188864 net.cpp:157] Top shape: 64 20 14 34 (609280)
I1130 19:45:21.976505 2760188864 net.cpp:165] Memory required for data: 13955328
I1130 19:45:21.976511 2760188864 layer_factory.hpp:77] Creating layer conv2
I1130 19:45:21.976523 2760188864 net.cpp:100] Creating Layer conv2
I1130 19:45:21.976531 2760188864 net.cpp:434] conv2 <- pool1
I1130 19:45:21.976541 2760188864 net.cpp:408] conv2 -> conv2
I1130 19:45:21.976912 2760188864 net.cpp:150] Setting up conv2
I1130 19:45:21.976922 2760188864 net.cpp:157] Top shape: 64 50 10 30 (960000)
I1130 19:45:21.976929 2760188864 net.cpp:165] Memory required for data: 17795328
I1130 19:45:21.976940 2760188864 layer_factory.hpp:77] Creating layer pool2
I1130 19:45:21.976950 2760188864 net.cpp:100] Creating Layer pool2
I1130 19:45:21.976958 2760188864 net.cpp:434] pool2 <- conv2
I1130 19:45:21.976966 2760188864 net.cpp:408] pool2 -> pool2
I1130 19:45:21.976979 2760188864 net.cpp:150] Setting up pool2
I1130 19:45:21.976986 2760188864 net.cpp:157] Top shape: 64 50 5 15 (240000)
I1130 19:45:21.976994 2760188864 net.cpp:165] Memory required for data: 18755328
I1130 19:45:21.977000 2760188864 layer_factory.hpp:77] Creating layer ip1
I1130 19:45:21.977011 2760188864 net.cpp:100] Creating Layer ip1
I1130 19:45:21.977018 2760188864 net.cpp:434] ip1 <- pool2
I1130 19:45:21.977027 2760188864 net.cpp:408] ip1 -> ip1
I1130 19:45:21.994618 2760188864 net.cpp:150] Setting up ip1
I1130 19:45:21.994647 2760188864 net.cpp:157] Top shape: 64 500 (32000)
I1130 19:45:21.994657 2760188864 net.cpp:165] Memory required for data: 18883328
I1130 19:45:21.994673 2760188864 layer_factory.hpp:77] Creating layer relu1
I1130 19:45:21.994691 2760188864 net.cpp:100] Creating Layer relu1
I1130 19:45:21.994699 2760188864 net.cpp:434] relu1 <- ip1
I1130 19:45:21.994709 2760188864 net.cpp:395] relu1 -> ip1 (in-place)
I1130 19:45:21.994720 2760188864 net.cpp:150] Setting up relu1
I1130 19:45:21.994727 2760188864 net.cpp:157] Top shape: 64 500 (32000)
I1130 19:45:21.994735 2760188864 net.cpp:165] Memory required for data: 19011328
I1130 19:45:21.994741 2760188864 layer_factory.hpp:77] Creating layer ip2
I1130 19:45:21.994752 2760188864 net.cpp:100] Creating Layer ip2
I1130 19:45:21.994758 2760188864 net.cpp:434] ip2 <- ip1
I1130 19:45:21.994767 2760188864 net.cpp:408] ip2 -> ip2
I1130 19:45:21.994920 2760188864 net.cpp:150] Setting up ip2
I1130 19:45:21.994928 2760188864 net.cpp:157] Top shape: 64 19 (1216)
I1130 19:45:21.994935 2760188864 net.cpp:165] Memory required for data: 19016192
I1130 19:45:21.994943 2760188864 layer_factory.hpp:77] Creating layer loss
I1130 19:45:21.994957 2760188864 net.cpp:100] Creating Layer loss
I1130 19:45:21.994964 2760188864 net.cpp:434] loss <- ip2
I1130 19:45:21.994971 2760188864 net.cpp:434] loss <- label
I1130 19:45:21.994982 2760188864 net.cpp:408] loss -> loss
I1130 19:45:21.994997 2760188864 layer_factory.hpp:77] Creating layer loss
I1130 19:45:21.995023 2760188864 net.cpp:150] Setting up loss
I1130 19:45:21.995030 2760188864 net.cpp:157] Top shape: (1)
I1130 19:45:21.995038 2760188864 net.cpp:160]     with loss weight 1
I1130 19:45:21.995059 2760188864 net.cpp:165] Memory required for data: 19016196
I1130 19:45:21.995066 2760188864 net.cpp:226] loss needs backward computation.
I1130 19:45:21.995074 2760188864 net.cpp:226] ip2 needs backward computation.
I1130 19:45:21.995080 2760188864 net.cpp:226] relu1 needs backward computation.
I1130 19:45:21.995086 2760188864 net.cpp:226] ip1 needs backward computation.
I1130 19:45:21.995093 2760188864 net.cpp:226] pool2 needs backward computation.
I1130 19:45:21.995106 2760188864 net.cpp:226] conv2 needs backward computation.
I1130 19:45:21.995113 2760188864 net.cpp:226] pool1 needs backward computation.
I1130 19:45:21.995120 2760188864 net.cpp:226] conv1 needs backward computation.
I1130 19:45:21.995148 2760188864 net.cpp:228] mnist does not need backward computation.
I1130 19:45:21.995157 2760188864 net.cpp:270] This network produces output loss
I1130 19:45:21.995167 2760188864 net.cpp:283] Network initialization done.
I1130 19:45:21.995471 2760188864 solver.cpp:181] Creating test net (#0) specified by net file: examples/mnist/lenet_train_math_test.prototxt
I1130 19:45:21.995510 2760188864 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I1130 19:45:21.995532 2760188864 net.cpp:58] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/imagenet/math_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 19
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1130 19:45:21.995712 2760188864 layer_factory.hpp:77] Creating layer mnist
I1130 19:45:21.995858 2760188864 net.cpp:100] Creating Layer mnist
I1130 19:45:21.995869 2760188864 net.cpp:408] mnist -> data
I1130 19:45:21.995882 2760188864 net.cpp:408] mnist -> label
I1130 19:45:21.995944 166072320 db_lmdb.cpp:35] Opened lmdb examples/imagenet/math_test_lmdb
I1130 19:45:21.995995 2760188864 data_layer.cpp:41] output data size: 100,3,32,72
I1130 19:45:22.000800 2760188864 net.cpp:150] Setting up mnist
I1130 19:45:22.000828 2760188864 net.cpp:157] Top shape: 100 3 32 72 (691200)
I1130 19:45:22.000838 2760188864 net.cpp:157] Top shape: 100 (100)
I1130 19:45:22.000844 2760188864 net.cpp:165] Memory required for data: 2765200
I1130 19:45:22.000852 2760188864 layer_factory.hpp:77] Creating layer label_mnist_1_split
I1130 19:45:22.000869 2760188864 net.cpp:100] Creating Layer label_mnist_1_split
I1130 19:45:22.000876 2760188864 net.cpp:434] label_mnist_1_split <- label
I1130 19:45:22.000887 2760188864 net.cpp:408] label_mnist_1_split -> label_mnist_1_split_0
I1130 19:45:22.000901 2760188864 net.cpp:408] label_mnist_1_split -> label_mnist_1_split_1
I1130 19:45:22.000913 2760188864 net.cpp:150] Setting up label_mnist_1_split
I1130 19:45:22.000919 2760188864 net.cpp:157] Top shape: 100 (100)
I1130 19:45:22.000926 2760188864 net.cpp:157] Top shape: 100 (100)
I1130 19:45:22.000932 2760188864 net.cpp:165] Memory required for data: 2766000
I1130 19:45:22.000972 2760188864 layer_factory.hpp:77] Creating layer conv1
I1130 19:45:22.000988 2760188864 net.cpp:100] Creating Layer conv1
I1130 19:45:22.000993 2760188864 net.cpp:434] conv1 <- data
I1130 19:45:22.001003 2760188864 net.cpp:408] conv1 -> conv1
I1130 19:45:22.001068 2760188864 net.cpp:150] Setting up conv1
I1130 19:45:22.001075 2760188864 net.cpp:157] Top shape: 100 20 28 68 (3808000)
I1130 19:45:22.001083 2760188864 net.cpp:165] Memory required for data: 17998000
I1130 19:45:22.001094 2760188864 layer_factory.hpp:77] Creating layer pool1
I1130 19:45:22.001103 2760188864 net.cpp:100] Creating Layer pool1
I1130 19:45:22.001109 2760188864 net.cpp:434] pool1 <- conv1
I1130 19:45:22.001116 2760188864 net.cpp:408] pool1 -> pool1
I1130 19:45:22.001129 2760188864 net.cpp:150] Setting up pool1
I1130 19:45:22.001135 2760188864 net.cpp:157] Top shape: 100 20 14 34 (952000)
I1130 19:45:22.001143 2760188864 net.cpp:165] Memory required for data: 21806000
I1130 19:45:22.001149 2760188864 layer_factory.hpp:77] Creating layer conv2
I1130 19:45:22.001166 2760188864 net.cpp:100] Creating Layer conv2
I1130 19:45:22.001173 2760188864 net.cpp:434] conv2 <- pool1
I1130 19:45:22.001181 2760188864 net.cpp:408] conv2 -> conv2
I1130 19:45:22.001569 2760188864 net.cpp:150] Setting up conv2
I1130 19:45:22.001576 2760188864 net.cpp:157] Top shape: 100 50 10 30 (1500000)
I1130 19:45:22.001585 2760188864 net.cpp:165] Memory required for data: 27806000
I1130 19:45:22.001593 2760188864 layer_factory.hpp:77] Creating layer pool2
I1130 19:45:22.001601 2760188864 net.cpp:100] Creating Layer pool2
I1130 19:45:22.001607 2760188864 net.cpp:434] pool2 <- conv2
I1130 19:45:22.001615 2760188864 net.cpp:408] pool2 -> pool2
I1130 19:45:22.001626 2760188864 net.cpp:150] Setting up pool2
I1130 19:45:22.001631 2760188864 net.cpp:157] Top shape: 100 50 5 15 (375000)
I1130 19:45:22.001639 2760188864 net.cpp:165] Memory required for data: 29306000
I1130 19:45:22.001646 2760188864 layer_factory.hpp:77] Creating layer ip1
I1130 19:45:22.001655 2760188864 net.cpp:100] Creating Layer ip1
I1130 19:45:22.001662 2760188864 net.cpp:434] ip1 <- pool2
I1130 19:45:22.001672 2760188864 net.cpp:408] ip1 -> ip1
I1130 19:45:22.022785 2760188864 net.cpp:150] Setting up ip1
I1130 19:45:22.022831 2760188864 net.cpp:157] Top shape: 100 500 (50000)
I1130 19:45:22.022848 2760188864 net.cpp:165] Memory required for data: 29506000
I1130 19:45:22.022869 2760188864 layer_factory.hpp:77] Creating layer relu1
I1130 19:45:22.022886 2760188864 net.cpp:100] Creating Layer relu1
I1130 19:45:22.022898 2760188864 net.cpp:434] relu1 <- ip1
I1130 19:45:22.022922 2760188864 net.cpp:395] relu1 -> ip1 (in-place)
I1130 19:45:22.022938 2760188864 net.cpp:150] Setting up relu1
I1130 19:45:22.022946 2760188864 net.cpp:157] Top shape: 100 500 (50000)
I1130 19:45:22.022953 2760188864 net.cpp:165] Memory required for data: 29706000
I1130 19:45:22.022970 2760188864 layer_factory.hpp:77] Creating layer ip2
I1130 19:45:22.022989 2760188864 net.cpp:100] Creating Layer ip2
I1130 19:45:22.023000 2760188864 net.cpp:434] ip2 <- ip1
I1130 19:45:22.023012 2760188864 net.cpp:408] ip2 -> ip2
I1130 19:45:22.023219 2760188864 net.cpp:150] Setting up ip2
I1130 19:45:22.023227 2760188864 net.cpp:157] Top shape: 100 19 (1900)
I1130 19:45:22.023236 2760188864 net.cpp:165] Memory required for data: 29713600
I1130 19:45:22.023248 2760188864 layer_factory.hpp:77] Creating layer ip2_ip2_0_split
I1130 19:45:22.023259 2760188864 net.cpp:100] Creating Layer ip2_ip2_0_split
I1130 19:45:22.023267 2760188864 net.cpp:434] ip2_ip2_0_split <- ip2
I1130 19:45:22.023290 2760188864 net.cpp:408] ip2_ip2_0_split -> ip2_ip2_0_split_0
I1130 19:45:22.023308 2760188864 net.cpp:408] ip2_ip2_0_split -> ip2_ip2_0_split_1
I1130 19:45:22.023320 2760188864 net.cpp:150] Setting up ip2_ip2_0_split
I1130 19:45:22.023327 2760188864 net.cpp:157] Top shape: 100 19 (1900)
I1130 19:45:22.023335 2760188864 net.cpp:157] Top shape: 100 19 (1900)
I1130 19:45:22.023341 2760188864 net.cpp:165] Memory required for data: 29728800
I1130 19:45:22.023406 2760188864 layer_factory.hpp:77] Creating layer accuracy
I1130 19:45:22.023429 2760188864 net.cpp:100] Creating Layer accuracy
I1130 19:45:22.023442 2760188864 net.cpp:434] accuracy <- ip2_ip2_0_split_0
I1130 19:45:22.023452 2760188864 net.cpp:434] accuracy <- label_mnist_1_split_0
I1130 19:45:22.023463 2760188864 net.cpp:408] accuracy -> accuracy
I1130 19:45:22.023486 2760188864 net.cpp:150] Setting up accuracy
I1130 19:45:22.023494 2760188864 net.cpp:157] Top shape: (1)
I1130 19:45:22.023500 2760188864 net.cpp:165] Memory required for data: 29728804
I1130 19:45:22.023509 2760188864 layer_factory.hpp:77] Creating layer loss
I1130 19:45:22.023519 2760188864 net.cpp:100] Creating Layer loss
I1130 19:45:22.023527 2760188864 net.cpp:434] loss <- ip2_ip2_0_split_1
I1130 19:45:22.023540 2760188864 net.cpp:434] loss <- label_mnist_1_split_1
I1130 19:45:22.023550 2760188864 net.cpp:408] loss -> loss
I1130 19:45:22.023572 2760188864 layer_factory.hpp:77] Creating layer loss
I1130 19:45:22.023620 2760188864 net.cpp:150] Setting up loss
I1130 19:45:22.023635 2760188864 net.cpp:157] Top shape: (1)
I1130 19:45:22.023644 2760188864 net.cpp:160]     with loss weight 1
I1130 19:45:22.023661 2760188864 net.cpp:165] Memory required for data: 29728808
I1130 19:45:22.023674 2760188864 net.cpp:226] loss needs backward computation.
I1130 19:45:22.023682 2760188864 net.cpp:228] accuracy does not need backward computation.
I1130 19:45:22.023689 2760188864 net.cpp:226] ip2_ip2_0_split needs backward computation.
I1130 19:45:22.023697 2760188864 net.cpp:226] ip2 needs backward computation.
I1130 19:45:22.023704 2760188864 net.cpp:226] relu1 needs backward computation.
I1130 19:45:22.023708 2760188864 net.cpp:226] ip1 needs backward computation.
I1130 19:45:22.023713 2760188864 net.cpp:226] pool2 needs backward computation.
I1130 19:45:22.023720 2760188864 net.cpp:226] conv2 needs backward computation.
I1130 19:45:22.023727 2760188864 net.cpp:226] pool1 needs backward computation.
I1130 19:45:22.023736 2760188864 net.cpp:226] conv1 needs backward computation.
I1130 19:45:22.023742 2760188864 net.cpp:228] label_mnist_1_split does not need backward computation.
I1130 19:45:22.023751 2760188864 net.cpp:228] mnist does not need backward computation.
I1130 19:45:22.023758 2760188864 net.cpp:270] This network produces output accuracy
I1130 19:45:22.023766 2760188864 net.cpp:270] This network produces output loss
I1130 19:45:22.023777 2760188864 net.cpp:283] Network initialization done.
I1130 19:45:22.023867 2760188864 solver.cpp:60] Solver scaffolding done.
I1130 19:45:22.023936 2760188864 caffe.cpp:251] Starting Optimization
I1130 19:45:22.023947 2760188864 solver.cpp:279] Solving LeNet
I1130 19:45:22.023952 2760188864 solver.cpp:280] Learning Rate Policy: inv
I1130 19:45:22.029536 2760188864 solver.cpp:337] Iteration 0, Testing net (#0)
I1130 19:45:30.134312 2760188864 solver.cpp:404]     Test net output #0: accuracy = 0.0748
I1130 19:45:30.134341 2760188864 solver.cpp:404]     Test net output #1: loss = 2.93947 (* 1 = 2.93947 loss)
I1130 19:45:30.281683 2760188864 solver.cpp:228] Iteration 0, loss = 2.96015
I1130 19:45:30.281714 2760188864 solver.cpp:244]     Train net output #0: loss = 2.96015 (* 1 = 2.96015 loss)
I1130 19:45:30.281749 2760188864 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1130 19:45:42.743494 2760188864 solver.cpp:228] Iteration 100, loss = 2.51824
I1130 19:45:42.743533 2760188864 solver.cpp:244]     Train net output #0: loss = 2.51824 (* 1 = 2.51824 loss)
I1130 19:45:42.743541 2760188864 sgd_solver.cpp:106] Iteration 100, lr = 0.00992565
I1130 19:45:55.154806 2760188864 solver.cpp:228] Iteration 200, loss = 2.35245
I1130 19:45:55.154855 2760188864 solver.cpp:244]     Train net output #0: loss = 2.35245 (* 1 = 2.35245 loss)
I1130 19:45:55.154867 2760188864 sgd_solver.cpp:106] Iteration 200, lr = 0.00985258
I1130 19:46:07.703367 2760188864 solver.cpp:228] Iteration 300, loss = 2.15146
I1130 19:46:07.703398 2760188864 solver.cpp:244]     Train net output #0: loss = 2.15146 (* 1 = 2.15146 loss)
I1130 19:46:07.703405 2760188864 sgd_solver.cpp:106] Iteration 300, lr = 0.00978075
I1130 19:46:21.258965 2760188864 solver.cpp:228] Iteration 400, loss = 1.63502
I1130 19:46:21.258997 2760188864 solver.cpp:244]     Train net output #0: loss = 1.63502 (* 1 = 1.63502 loss)
I1130 19:46:21.259007 2760188864 sgd_solver.cpp:106] Iteration 400, lr = 0.00971013
I1130 19:46:34.042990 2760188864 solver.cpp:337] Iteration 500, Testing net (#0)
I1130 19:46:42.824839 2760188864 solver.cpp:404]     Test net output #0: accuracy = 0.0084
I1130 19:46:42.824877 2760188864 solver.cpp:404]     Test net output #1: loss = 4.69246 (* 1 = 4.69246 loss)
I1130 19:46:42.964332 2760188864 solver.cpp:228] Iteration 500, loss = 1.27243
I1130 19:46:42.964366 2760188864 solver.cpp:244]     Train net output #0: loss = 1.27243 (* 1 = 1.27243 loss)
I1130 19:46:42.964377 2760188864 sgd_solver.cpp:106] Iteration 500, lr = 0.00964069
I1130 19:46:55.753864 2760188864 solver.cpp:228] Iteration 600, loss = 0.694175
I1130 19:46:55.753895 2760188864 solver.cpp:244]     Train net output #0: loss = 0.694175 (* 1 = 0.694175 loss)
I1130 19:46:55.753904 2760188864 sgd_solver.cpp:106] Iteration 600, lr = 0.0095724
I1130 19:47:08.668334 2760188864 solver.cpp:228] Iteration 700, loss = 0.328739
I1130 19:47:08.668385 2760188864 solver.cpp:244]     Train net output #0: loss = 0.328739 (* 1 = 0.328739 loss)
I1130 19:47:08.668396 2760188864 sgd_solver.cpp:106] Iteration 700, lr = 0.00950522
I1130 19:47:21.687206 2760188864 solver.cpp:228] Iteration 800, loss = 0.281664
I1130 19:47:21.687240 2760188864 solver.cpp:244]     Train net output #0: loss = 0.281664 (* 1 = 0.281664 loss)
I1130 19:47:21.687252 2760188864 sgd_solver.cpp:106] Iteration 800, lr = 0.00943913
I1130 19:47:35.702816 2760188864 solver.cpp:228] Iteration 900, loss = 0.0715628
I1130 19:47:35.702854 2760188864 solver.cpp:244]     Train net output #0: loss = 0.0715628 (* 1 = 0.0715628 loss)
I1130 19:47:35.702863 2760188864 sgd_solver.cpp:106] Iteration 900, lr = 0.00937411
I1130 19:47:49.351179 2760188864 solver.cpp:454] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I1130 19:47:49.408386 2760188864 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I1130 19:47:49.434383 2760188864 solver.cpp:337] Iteration 1000, Testing net (#0)
I1130 19:47:57.756403 2760188864 solver.cpp:404]     Test net output #0: accuracy = 0.1335
I1130 19:47:57.756434 2760188864 solver.cpp:404]     Test net output #1: loss = 8.11374 (* 1 = 8.11374 loss)
I1130 19:47:57.892150 2760188864 solver.cpp:228] Iteration 1000, loss = 0.0253262
I1130 19:47:57.892182 2760188864 solver.cpp:244]     Train net output #0: loss = 0.0253262 (* 1 = 0.0253262 loss)
I1130 19:47:57.892190 2760188864 sgd_solver.cpp:106] Iteration 1000, lr = 0.00931012
I1130 19:48:10.731426 2760188864 solver.cpp:228] Iteration 1100, loss = 0.0254424
I1130 19:48:10.731454 2760188864 solver.cpp:244]     Train net output #0: loss = 0.0254424 (* 1 = 0.0254424 loss)
I1130 19:48:10.731462 2760188864 sgd_solver.cpp:106] Iteration 1100, lr = 0.00924715
I1130 19:48:23.800954 2760188864 solver.cpp:228] Iteration 1200, loss = 0.00390877
I1130 19:48:23.801005 2760188864 solver.cpp:244]     Train net output #0: loss = 0.00390878 (* 1 = 0.00390878 loss)
I1130 19:48:23.801013 2760188864 sgd_solver.cpp:106] Iteration 1200, lr = 0.00918515
I1130 19:48:36.416126 2760188864 solver.cpp:228] Iteration 1300, loss = 0.00822753
I1130 19:48:36.416160 2760188864 solver.cpp:244]     Train net output #0: loss = 0.00822752 (* 1 = 0.00822752 loss)
I1130 19:48:36.416172 2760188864 sgd_solver.cpp:106] Iteration 1300, lr = 0.00912412
I1130 19:48:49.442103 2760188864 solver.cpp:228] Iteration 1400, loss = 0.00660089
I1130 19:48:49.442137 2760188864 solver.cpp:244]     Train net output #0: loss = 0.00660088 (* 1 = 0.00660088 loss)
I1130 19:48:49.442147 2760188864 sgd_solver.cpp:106] Iteration 1400, lr = 0.00906403
I1130 19:49:01.882380 2760188864 solver.cpp:337] Iteration 1500, Testing net (#0)
I1130 19:49:09.916147 2760188864 solver.cpp:404]     Test net output #0: accuracy = 0.1248
I1130 19:49:09.916178 2760188864 solver.cpp:404]     Test net output #1: loss = 9.06098 (* 1 = 9.06098 loss)
I1130 19:49:10.043485 2760188864 solver.cpp:228] Iteration 1500, loss = 0.00337732
I1130 19:49:10.043522 2760188864 solver.cpp:244]     Train net output #0: loss = 0.00337731 (* 1 = 0.00337731 loss)
I1130 19:49:10.043535 2760188864 sgd_solver.cpp:106] Iteration 1500, lr = 0.00900485
I1130 19:49:24.777724 2760188864 solver.cpp:228] Iteration 1600, loss = 0.00238007
I1130 19:49:24.777762 2760188864 solver.cpp:244]     Train net output #0: loss = 0.00238006 (* 1 = 0.00238006 loss)
I1130 19:49:24.777775 2760188864 sgd_solver.cpp:106] Iteration 1600, lr = 0.00894657
I1130 19:49:39.628669 2760188864 solver.cpp:228] Iteration 1700, loss = 0.00316241
I1130 19:49:39.628720 2760188864 solver.cpp:244]     Train net output #0: loss = 0.0031624 (* 1 = 0.0031624 loss)
I1130 19:49:39.628731 2760188864 sgd_solver.cpp:106] Iteration 1700, lr = 0.00888916
I1130 19:49:55.855641 2760188864 solver.cpp:228] Iteration 1800, loss = 0.00698599
I1130 19:49:55.855703 2760188864 solver.cpp:244]     Train net output #0: loss = 0.00698597 (* 1 = 0.00698597 loss)
I1130 19:49:55.855726 2760188864 sgd_solver.cpp:106] Iteration 1800, lr = 0.0088326
I1130 19:50:13.465508 2760188864 solver.cpp:228] Iteration 1900, loss = 0.00527212
I1130 19:50:13.465558 2760188864 solver.cpp:244]     Train net output #0: loss = 0.00527211 (* 1 = 0.00527211 loss)
I1130 19:50:13.465569 2760188864 sgd_solver.cpp:106] Iteration 1900, lr = 0.00877687
I1130 19:50:29.208123 2760188864 solver.cpp:454] Snapshotting to binary proto file examples/mnist/lenet_iter_2000.caffemodel
I1130 19:50:29.333212 2760188864 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_2000.solverstate
I1130 19:50:29.375145 2760188864 solver.cpp:337] Iteration 2000, Testing net (#0)
I1130 19:50:43.170078 2760188864 solver.cpp:404]     Test net output #0: accuracy = 0.1485
I1130 19:50:43.170125 2760188864 solver.cpp:404]     Test net output #1: loss = 9.0013 (* 1 = 9.0013 loss)
I1130 19:50:43.329634 2760188864 solver.cpp:228] Iteration 2000, loss = 0.00215712
I1130 19:50:43.329668 2760188864 solver.cpp:244]     Train net output #0: loss = 0.0021571 (* 1 = 0.0021571 loss)
I1130 19:50:43.329680 2760188864 sgd_solver.cpp:106] Iteration 2000, lr = 0.00872196
I1130 19:51:00.914839 2760188864 solver.cpp:228] Iteration 2100, loss = 0.00128737
I1130 19:51:00.914932 2760188864 solver.cpp:244]     Train net output #0: loss = 0.00128736 (* 1 = 0.00128736 loss)
I1130 19:51:00.914952 2760188864 sgd_solver.cpp:106] Iteration 2100, lr = 0.00866784
I1130 19:51:18.809902 2760188864 solver.cpp:228] Iteration 2200, loss = 0.00165785
I1130 19:51:18.809934 2760188864 solver.cpp:244]     Train net output #0: loss = 0.00165783 (* 1 = 0.00165783 loss)
I1130 19:51:18.809942 2760188864 sgd_solver.cpp:106] Iteration 2200, lr = 0.0086145
I1130 19:51:32.368393 2760188864 solver.cpp:228] Iteration 2300, loss = 0.000880841
I1130 19:51:32.368440 2760188864 solver.cpp:244]     Train net output #0: loss = 0.000880826 (* 1 = 0.000880826 loss)
I1130 19:51:32.368453 2760188864 sgd_solver.cpp:106] Iteration 2300, lr = 0.00856192
I1130 19:51:45.145339 2760188864 solver.cpp:228] Iteration 2400, loss = 0.00174189
I1130 19:51:45.145370 2760188864 solver.cpp:244]     Train net output #0: loss = 0.00174187 (* 1 = 0.00174187 loss)
I1130 19:51:45.145380 2760188864 sgd_solver.cpp:106] Iteration 2400, lr = 0.00851008
I1130 19:51:57.840960 2760188864 solver.cpp:337] Iteration 2500, Testing net (#0)
I1130 19:52:06.454030 2760188864 solver.cpp:404]     Test net output #0: accuracy = 0.1318
I1130 19:52:06.454078 2760188864 solver.cpp:404]     Test net output #1: loss = 8.95363 (* 1 = 8.95363 loss)
I1130 19:52:06.587424 2760188864 solver.cpp:228] Iteration 2500, loss = 0.00151506
I1130 19:52:06.587453 2760188864 solver.cpp:244]     Train net output #0: loss = 0.00151505 (* 1 = 0.00151505 loss)
I1130 19:52:06.587465 2760188864 sgd_solver.cpp:106] Iteration 2500, lr = 0.00845897
I1130 19:52:19.415892 2760188864 solver.cpp:228] Iteration 2600, loss = 0.00155737
I1130 19:52:19.415927 2760188864 solver.cpp:244]     Train net output #0: loss = 0.00155736 (* 1 = 0.00155736 loss)
I1130 19:52:19.415938 2760188864 sgd_solver.cpp:106] Iteration 2600, lr = 0.00840857
I1130 19:52:31.991065 2760188864 solver.cpp:228] Iteration 2700, loss = 0.00113179
I1130 19:52:31.991101 2760188864 solver.cpp:244]     Train net output #0: loss = 0.00113177 (* 1 = 0.00113177 loss)
I1130 19:52:31.991111 2760188864 sgd_solver.cpp:106] Iteration 2700, lr = 0.00835886
I1130 19:52:44.660711 2760188864 solver.cpp:228] Iteration 2800, loss = 0.00216243
I1130 19:52:44.661864 2760188864 solver.cpp:244]     Train net output #0: loss = 0.00216242 (* 1 = 0.00216242 loss)
I1130 19:52:44.661875 2760188864 sgd_solver.cpp:106] Iteration 2800, lr = 0.00830984
I1130 19:52:56.879654 2760188864 solver.cpp:228] Iteration 2900, loss = 0.00109076
I1130 19:52:56.879688 2760188864 solver.cpp:244]     Train net output #0: loss = 0.00109074 (* 1 = 0.00109074 loss)
I1130 19:52:56.879699 2760188864 sgd_solver.cpp:106] Iteration 2900, lr = 0.00826148
I1130 19:53:09.352150 2760188864 solver.cpp:454] Snapshotting to binary proto file examples/mnist/lenet_iter_3000.caffemodel
I1130 19:53:09.403627 2760188864 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_3000.solverstate
I1130 19:53:09.424558 2760188864 solver.cpp:337] Iteration 3000, Testing net (#0)
I1130 19:53:18.140097 2760188864 solver.cpp:404]     Test net output #0: accuracy = 0.1479
I1130 19:53:18.140146 2760188864 solver.cpp:404]     Test net output #1: loss = 8.80822 (* 1 = 8.80822 loss)
I1130 19:53:18.265168 2760188864 solver.cpp:228] Iteration 3000, loss = 0.0015698
I1130 19:53:18.265202 2760188864 solver.cpp:244]     Train net output #0: loss = 0.00156979 (* 1 = 0.00156979 loss)
I1130 19:53:18.265208 2760188864 sgd_solver.cpp:106] Iteration 3000, lr = 0.00821377
I1130 19:53:30.510850 2760188864 solver.cpp:228] Iteration 3100, loss = 0.00251249
I1130 19:53:30.510886 2760188864 solver.cpp:244]     Train net output #0: loss = 0.00251248 (* 1 = 0.00251248 loss)
I1130 19:53:30.510897 2760188864 sgd_solver.cpp:106] Iteration 3100, lr = 0.0081667
I1130 19:53:43.574832 2760188864 solver.cpp:228] Iteration 3200, loss = 0.00153306
I1130 19:53:43.574867 2760188864 solver.cpp:244]     Train net output #0: loss = 0.00153305 (* 1 = 0.00153305 loss)
I1130 19:53:43.574880 2760188864 sgd_solver.cpp:106] Iteration 3200, lr = 0.00812025
I1130 19:53:55.847513 2760188864 solver.cpp:228] Iteration 3300, loss = 0.00128508
I1130 19:53:55.847565 2760188864 solver.cpp:244]     Train net output #0: loss = 0.00128506 (* 1 = 0.00128506 loss)
I1130 19:53:55.847576 2760188864 sgd_solver.cpp:106] Iteration 3300, lr = 0.00807442
I1130 19:54:08.622803 2760188864 solver.cpp:228] Iteration 3400, loss = 0.00176897
I1130 19:54:08.622836 2760188864 solver.cpp:244]     Train net output #0: loss = 0.00176895 (* 1 = 0.00176895 loss)
I1130 19:54:08.622848 2760188864 sgd_solver.cpp:106] Iteration 3400, lr = 0.00802918
I1130 19:54:20.762222 2760188864 solver.cpp:337] Iteration 3500, Testing net (#0)
I1130 19:54:28.647563 2760188864 solver.cpp:404]     Test net output #0: accuracy = 0.1425
I1130 19:54:28.647614 2760188864 solver.cpp:404]     Test net output #1: loss = 8.75466 (* 1 = 8.75466 loss)
I1130 19:54:28.774024 2760188864 solver.cpp:228] Iteration 3500, loss = 0.00152389
I1130 19:54:28.774060 2760188864 solver.cpp:244]     Train net output #0: loss = 0.00152387 (* 1 = 0.00152387 loss)
I1130 19:54:28.774070 2760188864 sgd_solver.cpp:106] Iteration 3500, lr = 0.00798454
I1130 19:54:40.942397 2760188864 solver.cpp:228] Iteration 3600, loss = 0.00205055
I1130 19:54:40.942430 2760188864 solver.cpp:244]     Train net output #0: loss = 0.00205053 (* 1 = 0.00205053 loss)
I1130 19:54:40.942438 2760188864 sgd_solver.cpp:106] Iteration 3600, lr = 0.00794046
I1130 19:54:53.326485 2760188864 solver.cpp:228] Iteration 3700, loss = 0.0017186
I1130 19:54:53.326519 2760188864 solver.cpp:244]     Train net output #0: loss = 0.00171858 (* 1 = 0.00171858 loss)
I1130 19:54:53.326529 2760188864 sgd_solver.cpp:106] Iteration 3700, lr = 0.00789695
I1130 19:55:05.443159 2760188864 solver.cpp:228] Iteration 3800, loss = 0.00164655
I1130 19:55:05.443222 2760188864 solver.cpp:244]     Train net output #0: loss = 0.00164653 (* 1 = 0.00164653 loss)
I1130 19:55:05.443233 2760188864 sgd_solver.cpp:106] Iteration 3800, lr = 0.007854
I1130 19:55:17.625149 2760188864 solver.cpp:228] Iteration 3900, loss = 0.00136726
I1130 19:55:17.625182 2760188864 solver.cpp:244]     Train net output #0: loss = 0.00136725 (* 1 = 0.00136725 loss)
I1130 19:55:17.625193 2760188864 sgd_solver.cpp:106] Iteration 3900, lr = 0.00781158
I1130 19:55:29.762230 2760188864 solver.cpp:454] Snapshotting to binary proto file examples/mnist/lenet_iter_4000.caffemodel
I1130 19:55:29.812474 2760188864 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_4000.solverstate
I1130 19:55:29.837347 2760188864 solver.cpp:337] Iteration 4000, Testing net (#0)
I1130 19:55:37.739490 2760188864 solver.cpp:404]     Test net output #0: accuracy = 0.1389
I1130 19:55:37.739542 2760188864 solver.cpp:404]     Test net output #1: loss = 8.5764 (* 1 = 8.5764 loss)
I1130 19:55:37.865481 2760188864 solver.cpp:228] Iteration 4000, loss = 0.00152811
I1130 19:55:37.865525 2760188864 solver.cpp:244]     Train net output #0: loss = 0.00152809 (* 1 = 0.00152809 loss)
I1130 19:55:37.865541 2760188864 sgd_solver.cpp:106] Iteration 4000, lr = 0.0077697
I1130 19:55:50.049859 2760188864 solver.cpp:228] Iteration 4100, loss = 0.00157175
I1130 19:55:50.049891 2760188864 solver.cpp:244]     Train net output #0: loss = 0.00157173 (* 1 = 0.00157173 loss)
I1130 19:55:50.049902 2760188864 sgd_solver.cpp:106] Iteration 4100, lr = 0.00772833
I1130 19:56:02.354084 2760188864 solver.cpp:228] Iteration 4200, loss = 0.00201894
I1130 19:56:02.354120 2760188864 solver.cpp:244]     Train net output #0: loss = 0.00201892 (* 1 = 0.00201892 loss)
I1130 19:56:02.354131 2760188864 sgd_solver.cpp:106] Iteration 4200, lr = 0.00768748
I1130 19:56:14.513391 2760188864 solver.cpp:228] Iteration 4300, loss = 0.00156685
I1130 19:56:14.513443 2760188864 solver.cpp:244]     Train net output #0: loss = 0.00156684 (* 1 = 0.00156684 loss)
I1130 19:56:14.513455 2760188864 sgd_solver.cpp:106] Iteration 4300, lr = 0.00764712
I1130 19:56:26.637426 2760188864 solver.cpp:228] Iteration 4400, loss = 0.00185769
I1130 19:56:26.637459 2760188864 solver.cpp:244]     Train net output #0: loss = 0.00185768 (* 1 = 0.00185768 loss)
I1130 19:56:26.637467 2760188864 sgd_solver.cpp:106] Iteration 4400, lr = 0.00760726
I1130 19:56:38.724614 2760188864 solver.cpp:337] Iteration 4500, Testing net (#0)
I1130 19:56:46.657999 2760188864 solver.cpp:404]     Test net output #0: accuracy = 0.1403
I1130 19:56:46.658052 2760188864 solver.cpp:404]     Test net output #1: loss = 8.40689 (* 1 = 8.40689 loss)
I1130 19:56:46.784636 2760188864 solver.cpp:228] Iteration 4500, loss = 0.00214725
I1130 19:56:46.784668 2760188864 solver.cpp:244]     Train net output #0: loss = 0.00214723 (* 1 = 0.00214723 loss)
I1130 19:56:46.784678 2760188864 sgd_solver.cpp:106] Iteration 4500, lr = 0.00756788
I1130 19:56:58.889938 2760188864 solver.cpp:228] Iteration 4600, loss = 0.00171519
I1130 19:56:58.889973 2760188864 solver.cpp:244]     Train net output #0: loss = 0.00171517 (* 1 = 0.00171517 loss)
I1130 19:56:58.889986 2760188864 sgd_solver.cpp:106] Iteration 4600, lr = 0.00752897
I1130 19:57:11.087019 2760188864 solver.cpp:228] Iteration 4700, loss = 0.00185523
I1130 19:57:11.087054 2760188864 solver.cpp:244]     Train net output #0: loss = 0.00185521 (* 1 = 0.00185521 loss)
I1130 19:57:11.087064 2760188864 sgd_solver.cpp:106] Iteration 4700, lr = 0.00749052
I1130 19:57:23.301434 2760188864 solver.cpp:228] Iteration 4800, loss = 0.00257299
I1130 19:57:23.301496 2760188864 solver.cpp:244]     Train net output #0: loss = 0.00257298 (* 1 = 0.00257298 loss)
I1130 19:57:23.301508 2760188864 sgd_solver.cpp:106] Iteration 4800, lr = 0.00745253
I1130 19:57:35.550927 2760188864 solver.cpp:228] Iteration 4900, loss = 0.0026891
I1130 19:57:35.550961 2760188864 solver.cpp:244]     Train net output #0: loss = 0.00268908 (* 1 = 0.00268908 loss)
I1130 19:57:35.550969 2760188864 sgd_solver.cpp:106] Iteration 4900, lr = 0.00741498
I1130 19:57:47.736696 2760188864 solver.cpp:454] Snapshotting to binary proto file examples/mnist/lenet_iter_5000.caffemodel
I1130 19:57:47.789731 2760188864 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_5000.solverstate
I1130 19:57:47.813371 2760188864 solver.cpp:337] Iteration 5000, Testing net (#0)
I1130 19:57:55.758095 2760188864 solver.cpp:404]     Test net output #0: accuracy = 0.1433
I1130 19:57:55.758153 2760188864 solver.cpp:404]     Test net output #1: loss = 8.37495 (* 1 = 8.37495 loss)
I1130 19:57:55.887346 2760188864 solver.cpp:228] Iteration 5000, loss = 0.00188285
I1130 19:57:55.887380 2760188864 solver.cpp:244]     Train net output #0: loss = 0.00188284 (* 1 = 0.00188284 loss)
I1130 19:57:55.887392 2760188864 sgd_solver.cpp:106] Iteration 5000, lr = 0.00737788
I1130 19:58:08.058006 2760188864 solver.cpp:228] Iteration 5100, loss = 0.00128876
I1130 19:58:08.058039 2760188864 solver.cpp:244]     Train net output #0: loss = 0.00128874 (* 1 = 0.00128874 loss)
I1130 19:58:08.058050 2760188864 sgd_solver.cpp:106] Iteration 5100, lr = 0.0073412
I1130 19:58:20.302839 2760188864 solver.cpp:228] Iteration 5200, loss = 0.00130505
I1130 19:58:20.302872 2760188864 solver.cpp:244]     Train net output #0: loss = 0.00130503 (* 1 = 0.00130503 loss)
I1130 19:58:20.302883 2760188864 sgd_solver.cpp:106] Iteration 5200, lr = 0.00730495
I1130 19:58:32.593291 2760188864 solver.cpp:228] Iteration 5300, loss = 0.00193502
I1130 19:58:32.593340 2760188864 solver.cpp:244]     Train net output #0: loss = 0.001935 (* 1 = 0.001935 loss)
I1130 19:58:32.593350 2760188864 sgd_solver.cpp:106] Iteration 5300, lr = 0.00726911
I1130 19:58:44.787953 2760188864 solver.cpp:228] Iteration 5400, loss = 0.00206992
I1130 19:58:44.787988 2760188864 solver.cpp:244]     Train net output #0: loss = 0.0020699 (* 1 = 0.0020699 loss)
I1130 19:58:44.788000 2760188864 sgd_solver.cpp:106] Iteration 5400, lr = 0.00723368
I1130 19:58:56.928478 2760188864 solver.cpp:337] Iteration 5500, Testing net (#0)
I1130 19:59:04.811532 2760188864 solver.cpp:404]     Test net output #0: accuracy = 0.1429
I1130 19:59:04.811581 2760188864 solver.cpp:404]     Test net output #1: loss = 8.2991 (* 1 = 8.2991 loss)
I1130 19:59:04.934909 2760188864 solver.cpp:228] Iteration 5500, loss = 0.00117278
I1130 19:59:04.934942 2760188864 solver.cpp:244]     Train net output #0: loss = 0.00117277 (* 1 = 0.00117277 loss)
I1130 19:59:04.934950 2760188864 sgd_solver.cpp:106] Iteration 5500, lr = 0.00719865
I1130 19:59:17.104861 2760188864 solver.cpp:228] Iteration 5600, loss = 0.0021665
I1130 19:59:17.104892 2760188864 solver.cpp:244]     Train net output #0: loss = 0.00216648 (* 1 = 0.00216648 loss)
I1130 19:59:17.104902 2760188864 sgd_solver.cpp:106] Iteration 5600, lr = 0.00716402
I1130 19:59:29.369184 2760188864 solver.cpp:228] Iteration 5700, loss = 0.00176324
I1130 19:59:29.369217 2760188864 solver.cpp:244]     Train net output #0: loss = 0.00176322 (* 1 = 0.00176322 loss)
I1130 19:59:29.369228 2760188864 sgd_solver.cpp:106] Iteration 5700, lr = 0.00712977
I1130 19:59:41.491000 2760188864 solver.cpp:228] Iteration 5800, loss = 0.00260493
I1130 19:59:41.492063 2760188864 solver.cpp:244]     Train net output #0: loss = 0.00260491 (* 1 = 0.00260491 loss)
I1130 19:59:41.492074 2760188864 sgd_solver.cpp:106] Iteration 5800, lr = 0.0070959
I1130 19:59:53.666810 2760188864 solver.cpp:228] Iteration 5900, loss = 0.0015012
I1130 19:59:53.666846 2760188864 solver.cpp:244]     Train net output #0: loss = 0.00150118 (* 1 = 0.00150118 loss)
I1130 19:59:53.666854 2760188864 sgd_solver.cpp:106] Iteration 5900, lr = 0.0070624
I1130 20:00:05.761262 2760188864 solver.cpp:454] Snapshotting to binary proto file examples/mnist/lenet_iter_6000.caffemodel
I1130 20:00:05.818295 2760188864 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_6000.solverstate
I1130 20:00:05.840549 2760188864 solver.cpp:337] Iteration 6000, Testing net (#0)
I1130 20:00:13.788482 2760188864 solver.cpp:404]     Test net output #0: accuracy = 0.1487
I1130 20:00:13.790838 2760188864 solver.cpp:404]     Test net output #1: loss = 8.24727 (* 1 = 8.24727 loss)
I1130 20:00:13.913877 2760188864 solver.cpp:228] Iteration 6000, loss = 0.00152185
I1130 20:00:13.913907 2760188864 solver.cpp:244]     Train net output #0: loss = 0.00152184 (* 1 = 0.00152184 loss)
I1130 20:00:13.913914 2760188864 sgd_solver.cpp:106] Iteration 6000, lr = 0.00702927
I1130 20:00:26.100121 2760188864 solver.cpp:228] Iteration 6100, loss = 0.00141132
I1130 20:00:26.100154 2760188864 solver.cpp:244]     Train net output #0: loss = 0.0014113 (* 1 = 0.0014113 loss)
I1130 20:00:26.100162 2760188864 sgd_solver.cpp:106] Iteration 6100, lr = 0.0069965
I1130 20:00:38.296586 2760188864 solver.cpp:228] Iteration 6200, loss = 0.0027587
I1130 20:00:38.296620 2760188864 solver.cpp:244]     Train net output #0: loss = 0.00275868 (* 1 = 0.00275868 loss)
I1130 20:00:38.296633 2760188864 sgd_solver.cpp:106] Iteration 6200, lr = 0.00696408
I1130 20:00:50.484097 2760188864 solver.cpp:228] Iteration 6300, loss = 0.0017782
I1130 20:00:50.484150 2760188864 solver.cpp:244]     Train net output #0: loss = 0.00177818 (* 1 = 0.00177818 loss)
I1130 20:00:50.484161 2760188864 sgd_solver.cpp:106] Iteration 6300, lr = 0.00693201
I1130 20:01:02.660341 2760188864 solver.cpp:228] Iteration 6400, loss = 0.00153175
I1130 20:01:02.660377 2760188864 solver.cpp:244]     Train net output #0: loss = 0.00153173 (* 1 = 0.00153173 loss)
I1130 20:01:02.660387 2760188864 sgd_solver.cpp:106] Iteration 6400, lr = 0.00690029
I1130 20:01:14.856976 2760188864 solver.cpp:337] Iteration 6500, Testing net (#0)
I1130 20:01:22.739120 2760188864 solver.cpp:404]     Test net output #0: accuracy = 0.1524
I1130 20:01:22.739163 2760188864 solver.cpp:404]     Test net output #1: loss = 8.13719 (* 1 = 8.13719 loss)
I1130 20:01:22.862913 2760188864 solver.cpp:228] Iteration 6500, loss = 0.00150733
I1130 20:01:22.862946 2760188864 solver.cpp:244]     Train net output #0: loss = 0.00150731 (* 1 = 0.00150731 loss)
I1130 20:01:22.862957 2760188864 sgd_solver.cpp:106] Iteration 6500, lr = 0.0068689
I1130 20:01:35.040284 2760188864 solver.cpp:228] Iteration 6600, loss = 0.00273742
I1130 20:01:35.040314 2760188864 solver.cpp:244]     Train net output #0: loss = 0.0027374 (* 1 = 0.0027374 loss)
I1130 20:01:35.040323 2760188864 sgd_solver.cpp:106] Iteration 6600, lr = 0.00683784
I1130 20:01:47.371644 2760188864 solver.cpp:228] Iteration 6700, loss = 0.00305138
I1130 20:01:47.371677 2760188864 solver.cpp:244]     Train net output #0: loss = 0.00305136 (* 1 = 0.00305136 loss)
I1130 20:01:47.371688 2760188864 sgd_solver.cpp:106] Iteration 6700, lr = 0.00680711
I1130 20:01:59.529844 2760188864 solver.cpp:228] Iteration 6800, loss = 0.00161195
I1130 20:01:59.530972 2760188864 solver.cpp:244]     Train net output #0: loss = 0.00161193 (* 1 = 0.00161193 loss)
I1130 20:01:59.530983 2760188864 sgd_solver.cpp:106] Iteration 6800, lr = 0.0067767
I1130 20:02:11.678179 2760188864 solver.cpp:228] Iteration 6900, loss = 0.00130859
I1130 20:02:11.678213 2760188864 solver.cpp:244]     Train net output #0: loss = 0.00130857 (* 1 = 0.00130857 loss)
I1130 20:02:11.678225 2760188864 sgd_solver.cpp:106] Iteration 6900, lr = 0.0067466
I1130 20:02:23.836184 2760188864 solver.cpp:454] Snapshotting to binary proto file examples/mnist/lenet_iter_7000.caffemodel
I1130 20:02:23.892823 2760188864 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_7000.solverstate
I1130 20:02:23.916892 2760188864 solver.cpp:337] Iteration 7000, Testing net (#0)
I1130 20:02:31.831830 2760188864 solver.cpp:404]     Test net output #0: accuracy = 0.159
I1130 20:02:31.831897 2760188864 solver.cpp:404]     Test net output #1: loss = 8.09248 (* 1 = 8.09248 loss)
I1130 20:02:31.958673 2760188864 solver.cpp:228] Iteration 7000, loss = 0.0016949
I1130 20:02:31.958706 2760188864 solver.cpp:244]     Train net output #0: loss = 0.00169489 (* 1 = 0.00169489 loss)
I1130 20:02:31.958717 2760188864 sgd_solver.cpp:106] Iteration 7000, lr = 0.00671681
I1130 20:02:44.194986 2760188864 solver.cpp:228] Iteration 7100, loss = 0.00161622
I1130 20:02:44.195020 2760188864 solver.cpp:244]     Train net output #0: loss = 0.0016162 (* 1 = 0.0016162 loss)
I1130 20:02:44.195032 2760188864 sgd_solver.cpp:106] Iteration 7100, lr = 0.00668733
I1130 20:02:56.437983 2760188864 solver.cpp:228] Iteration 7200, loss = 0.00205561
I1130 20:02:56.438019 2760188864 solver.cpp:244]     Train net output #0: loss = 0.00205559 (* 1 = 0.00205559 loss)
I1130 20:02:56.438030 2760188864 sgd_solver.cpp:106] Iteration 7200, lr = 0.00665815
I1130 20:03:08.604116 2760188864 solver.cpp:228] Iteration 7300, loss = 0.00235853
I1130 20:03:08.605183 2760188864 solver.cpp:244]     Train net output #0: loss = 0.00235851 (* 1 = 0.00235851 loss)
I1130 20:03:08.605195 2760188864 sgd_solver.cpp:106] Iteration 7300, lr = 0.00662927
I1130 20:03:20.828639 2760188864 solver.cpp:228] Iteration 7400, loss = 0.00127453
I1130 20:03:20.828672 2760188864 solver.cpp:244]     Train net output #0: loss = 0.00127451 (* 1 = 0.00127451 loss)
I1130 20:03:20.828680 2760188864 sgd_solver.cpp:106] Iteration 7400, lr = 0.00660067
I1130 20:03:32.938009 2760188864 solver.cpp:337] Iteration 7500, Testing net (#0)
I1130 20:03:40.905972 2760188864 solver.cpp:404]     Test net output #0: accuracy = 0.1568
I1130 20:03:40.906985 2760188864 solver.cpp:404]     Test net output #1: loss = 7.89977 (* 1 = 7.89977 loss)
I1130 20:03:41.031921 2760188864 solver.cpp:228] Iteration 7500, loss = 0.00226569
I1130 20:03:41.031957 2760188864 solver.cpp:244]     Train net output #0: loss = 0.00226567 (* 1 = 0.00226567 loss)
I1130 20:03:41.031967 2760188864 sgd_solver.cpp:106] Iteration 7500, lr = 0.00657236
I1130 20:03:53.238543 2760188864 solver.cpp:228] Iteration 7600, loss = 0.00227958
I1130 20:03:53.238577 2760188864 solver.cpp:244]     Train net output #0: loss = 0.00227956 (* 1 = 0.00227956 loss)
I1130 20:03:53.238589 2760188864 sgd_solver.cpp:106] Iteration 7600, lr = 0.00654433
I1130 20:04:05.577069 2760188864 solver.cpp:228] Iteration 7700, loss = 0.0013735
I1130 20:04:05.577100 2760188864 solver.cpp:244]     Train net output #0: loss = 0.00137348 (* 1 = 0.00137348 loss)
I1130 20:04:05.577110 2760188864 sgd_solver.cpp:106] Iteration 7700, lr = 0.00651658
I1130 20:04:17.786861 2760188864 solver.cpp:228] Iteration 7800, loss = 0.00206024
I1130 20:04:17.787937 2760188864 solver.cpp:244]     Train net output #0: loss = 0.00206023 (* 1 = 0.00206023 loss)
I1130 20:04:17.787950 2760188864 sgd_solver.cpp:106] Iteration 7800, lr = 0.00648911
I1130 20:04:29.961694 2760188864 solver.cpp:228] Iteration 7900, loss = 0.00235007
I1130 20:04:29.961729 2760188864 solver.cpp:244]     Train net output #0: loss = 0.00235005 (* 1 = 0.00235005 loss)
I1130 20:04:29.961738 2760188864 sgd_solver.cpp:106] Iteration 7900, lr = 0.0064619
I1130 20:04:42.203308 2760188864 solver.cpp:454] Snapshotting to binary proto file examples/mnist/lenet_iter_8000.caffemodel
I1130 20:04:42.258882 2760188864 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_8000.solverstate
I1130 20:04:42.285817 2760188864 solver.cpp:337] Iteration 8000, Testing net (#0)
I1130 20:04:50.310941 2760188864 solver.cpp:404]     Test net output #0: accuracy = 0.1575
I1130 20:04:50.312018 2760188864 solver.cpp:404]     Test net output #1: loss = 7.93236 (* 1 = 7.93236 loss)
I1130 20:04:50.437660 2760188864 solver.cpp:228] Iteration 8000, loss = 0.00167566
I1130 20:04:50.437695 2760188864 solver.cpp:244]     Train net output #0: loss = 0.00167564 (* 1 = 0.00167564 loss)
I1130 20:04:50.437705 2760188864 sgd_solver.cpp:106] Iteration 8000, lr = 0.00643496
I1130 20:05:02.582731 2760188864 solver.cpp:228] Iteration 8100, loss = 0.00253993
I1130 20:05:02.582764 2760188864 solver.cpp:244]     Train net output #0: loss = 0.00253991 (* 1 = 0.00253991 loss)
I1130 20:05:02.582775 2760188864 sgd_solver.cpp:106] Iteration 8100, lr = 0.00640827
I1130 20:05:14.920922 2760188864 solver.cpp:228] Iteration 8200, loss = 0.0018405
I1130 20:05:14.920955 2760188864 solver.cpp:244]     Train net output #0: loss = 0.00184048 (* 1 = 0.00184048 loss)
I1130 20:05:14.920964 2760188864 sgd_solver.cpp:106] Iteration 8200, lr = 0.00638185
I1130 20:05:27.154697 2760188864 solver.cpp:228] Iteration 8300, loss = 0.00183834
I1130 20:05:27.155863 2760188864 solver.cpp:244]     Train net output #0: loss = 0.00183832 (* 1 = 0.00183832 loss)
I1130 20:05:27.155874 2760188864 sgd_solver.cpp:106] Iteration 8300, lr = 0.00635567
I1130 20:05:39.892810 2760188864 solver.cpp:228] Iteration 8400, loss = 0.00218854
I1130 20:05:39.892844 2760188864 solver.cpp:244]     Train net output #0: loss = 0.00218852 (* 1 = 0.00218852 loss)
I1130 20:05:39.892854 2760188864 sgd_solver.cpp:106] Iteration 8400, lr = 0.00632975
I1130 20:05:52.203420 2760188864 solver.cpp:337] Iteration 8500, Testing net (#0)
I1130 20:06:00.109347 2760188864 solver.cpp:404]     Test net output #0: accuracy = 0.1563
I1130 20:06:00.110396 2760188864 solver.cpp:404]     Test net output #1: loss = 7.91583 (* 1 = 7.91583 loss)
I1130 20:06:00.238045 2760188864 solver.cpp:228] Iteration 8500, loss = 0.00186396
I1130 20:06:00.238081 2760188864 solver.cpp:244]     Train net output #0: loss = 0.00186395 (* 1 = 0.00186395 loss)
I1130 20:06:00.238093 2760188864 sgd_solver.cpp:106] Iteration 8500, lr = 0.00630407
I1130 20:06:12.454838 2760188864 solver.cpp:228] Iteration 8600, loss = 0.00261045
I1130 20:06:12.454876 2760188864 solver.cpp:244]     Train net output #0: loss = 0.00261043 (* 1 = 0.00261043 loss)
I1130 20:06:12.454885 2760188864 sgd_solver.cpp:106] Iteration 8600, lr = 0.00627864
I1130 20:06:24.786036 2760188864 solver.cpp:228] Iteration 8700, loss = 0.00177538
I1130 20:06:24.786070 2760188864 solver.cpp:244]     Train net output #0: loss = 0.00177537 (* 1 = 0.00177537 loss)
I1130 20:06:24.786078 2760188864 sgd_solver.cpp:106] Iteration 8700, lr = 0.00625344
I1130 20:06:36.944016 2760188864 solver.cpp:228] Iteration 8800, loss = 0.00247114
I1130 20:06:36.945077 2760188864 solver.cpp:244]     Train net output #0: loss = 0.00247113 (* 1 = 0.00247113 loss)
I1130 20:06:36.945087 2760188864 sgd_solver.cpp:106] Iteration 8800, lr = 0.00622847
I1130 20:06:49.207012 2760188864 solver.cpp:228] Iteration 8900, loss = 0.00252539
I1130 20:06:49.207047 2760188864 solver.cpp:244]     Train net output #0: loss = 0.00252537 (* 1 = 0.00252537 loss)
I1130 20:06:49.207058 2760188864 sgd_solver.cpp:106] Iteration 8900, lr = 0.00620374
I1130 20:07:01.449126 2760188864 solver.cpp:454] Snapshotting to binary proto file examples/mnist/lenet_iter_9000.caffemodel
I1130 20:07:01.512135 2760188864 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_9000.solverstate
I1130 20:07:01.536620 2760188864 solver.cpp:337] Iteration 9000, Testing net (#0)
I1130 20:07:09.464165 2760188864 solver.cpp:404]     Test net output #0: accuracy = 0.1599
I1130 20:07:09.465262 2760188864 solver.cpp:404]     Test net output #1: loss = 7.76451 (* 1 = 7.76451 loss)
I1130 20:07:09.591883 2760188864 solver.cpp:228] Iteration 9000, loss = 0.00257965
I1130 20:07:09.591917 2760188864 solver.cpp:244]     Train net output #0: loss = 0.00257963 (* 1 = 0.00257963 loss)
I1130 20:07:09.591929 2760188864 sgd_solver.cpp:106] Iteration 9000, lr = 0.00617924
I1130 20:07:21.852545 2760188864 solver.cpp:228] Iteration 9100, loss = 0.00173692
I1130 20:07:21.852579 2760188864 solver.cpp:244]     Train net output #0: loss = 0.0017369 (* 1 = 0.0017369 loss)
I1130 20:07:21.852587 2760188864 sgd_solver.cpp:106] Iteration 9100, lr = 0.00615496
I1130 20:07:34.128618 2760188864 solver.cpp:228] Iteration 9200, loss = 0.00246744
I1130 20:07:34.128654 2760188864 solver.cpp:244]     Train net output #0: loss = 0.00246743 (* 1 = 0.00246743 loss)
I1130 20:07:34.128664 2760188864 sgd_solver.cpp:106] Iteration 9200, lr = 0.0061309
I1130 20:07:46.346931 2760188864 solver.cpp:228] Iteration 9300, loss = 0.00283818
I1130 20:07:46.349185 2760188864 solver.cpp:244]     Train net output #0: loss = 0.00283816 (* 1 = 0.00283816 loss)
I1130 20:07:46.349197 2760188864 sgd_solver.cpp:106] Iteration 9300, lr = 0.00610706
I1130 20:07:58.479936 2760188864 solver.cpp:228] Iteration 9400, loss = 0.00111446
I1130 20:07:58.479970 2760188864 solver.cpp:244]     Train net output #0: loss = 0.00111444 (* 1 = 0.00111444 loss)
I1130 20:07:58.479980 2760188864 sgd_solver.cpp:106] Iteration 9400, lr = 0.00608343
I1130 20:08:10.607955 2760188864 solver.cpp:337] Iteration 9500, Testing net (#0)
I1130 20:08:18.604918 2760188864 solver.cpp:404]     Test net output #0: accuracy = 0.1568
I1130 20:08:18.606058 2760188864 solver.cpp:404]     Test net output #1: loss = 7.85129 (* 1 = 7.85129 loss)
I1130 20:08:18.736048 2760188864 solver.cpp:228] Iteration 9500, loss = 0.0022809
I1130 20:08:18.736083 2760188864 solver.cpp:244]     Train net output #0: loss = 0.00228088 (* 1 = 0.00228088 loss)
I1130 20:08:18.736091 2760188864 sgd_solver.cpp:106] Iteration 9500, lr = 0.00606002
I1130 20:08:30.890740 2760188864 solver.cpp:228] Iteration 9600, loss = 0.00217172
I1130 20:08:30.890776 2760188864 solver.cpp:244]     Train net output #0: loss = 0.0021717 (* 1 = 0.0021717 loss)
I1130 20:08:30.890787 2760188864 sgd_solver.cpp:106] Iteration 9600, lr = 0.00603682
I1130 20:08:43.179653 2760188864 solver.cpp:228] Iteration 9700, loss = 0.0020112
I1130 20:08:43.179687 2760188864 solver.cpp:244]     Train net output #0: loss = 0.00201118 (* 1 = 0.00201118 loss)
I1130 20:08:43.179697 2760188864 sgd_solver.cpp:106] Iteration 9700, lr = 0.00601382
I1130 20:08:55.382985 2760188864 solver.cpp:228] Iteration 9800, loss = 0.00184312
I1130 20:08:55.384057 2760188864 solver.cpp:244]     Train net output #0: loss = 0.00184311 (* 1 = 0.00184311 loss)
I1130 20:08:55.384068 2760188864 sgd_solver.cpp:106] Iteration 9800, lr = 0.00599102
I1130 20:09:07.713027 2760188864 solver.cpp:228] Iteration 9900, loss = 0.00224067
I1130 20:09:07.713062 2760188864 solver.cpp:244]     Train net output #0: loss = 0.00224066 (* 1 = 0.00224066 loss)
I1130 20:09:07.713071 2760188864 sgd_solver.cpp:106] Iteration 9900, lr = 0.00596843
I1130 20:09:19.960695 2760188864 solver.cpp:454] Snapshotting to binary proto file examples/mnist/lenet_iter_10000.caffemodel
I1130 20:09:20.018126 2760188864 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_10000.solverstate
I1130 20:09:20.119341 2760188864 solver.cpp:317] Iteration 10000, loss = 0.00198115
I1130 20:09:20.119371 2760188864 solver.cpp:337] Iteration 10000, Testing net (#0)
I1130 20:09:28.040396 2760188864 solver.cpp:404]     Test net output #0: accuracy = 0.1577
I1130 20:09:28.041476 2760188864 solver.cpp:404]     Test net output #1: loss = 7.79537 (* 1 = 7.79537 loss)
I1130 20:09:28.041486 2760188864 solver.cpp:322] Optimization Done.
I1130 20:09:28.041504 2760188864 caffe.cpp:254] Optimization Done.
